{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f4055ae-e152-4385-952c-08b22743e938",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validate_and_prepare_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 127\u001b[0m\n\u001b[1;32m    124\u001b[0m train_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/titanic/titanic_train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    125\u001b[0m test_file \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/titanic/titanic_test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 127\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m validate_and_prepare_data(train_file, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    128\u001b[0m X_train_split, X_val_split, y_train_split, y_val_split \u001b[38;5;241m=\u001b[39m split_train_test(X_train, y_train)\n\u001b[1;32m    129\u001b[0m X_test \u001b[38;5;241m=\u001b[39m validate_and_prepare_data(test_file, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'validate_and_prepare_data' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Dict, Any\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "model_dict = {\n",
    "    'logistic_regression': LogisticRegression,\n",
    "    'svm': LinearSVC,\n",
    "    'random_forest': RandomForestClassifier,\n",
    "    'knn': KNeighborsClassifier,\n",
    "    'decision_tree': DecisionTreeClassifier,\n",
    "}\n",
    "\n",
    "\n",
    "default_params_dict = {\n",
    "    'logistic_regression': {'C': 1, 'max_iter': 100},\n",
    "    'svm': {'C': 1, 'kernel': 'rbf'},\n",
    "    'random_forest': {'n_estimators': 100, 'max_depth': None},\n",
    "    'knn': {'n_neighbors': 5, 'algorithm': 'auto'},\n",
    "    'decision_tree': {'max_depth': None},\n",
    "}\n",
    "\n",
    "MODEL_DIR = 'models'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def split_train_test(\n",
    "        X: pd.DataFrame,\n",
    "        y: pd.Series,\n",
    "        test_size: float = 0.2,\n",
    "        random_state: int = 42\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Splits the training data into a training and validation set.\n",
    "\n",
    "    :param X: Feature matrix.\n",
    "    :param y: Target variable.\n",
    "    :param test_size: Proportion of data to be used as the validation set.\n",
    "    :param random_state: Random seed for reproducibility.\n",
    "    :return: Train and validation sets (X_train, X_val, y_train, y_val).\n",
    "    \"\"\"\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "\n",
    "def create_model(model_type: str,\n",
    "                 model_params: Optional[Dict[str, Any]] = None) -> make_pipeline:\n",
    "    \"\"\"\n",
    "    Creates a pipeline with the specified model.\n",
    "\n",
    "    :param model_type: The type of model, either 'logistic_regression', 'svm', 'random_forest', etc.\n",
    "    :param model_params: A dictionary of hyperparameters for the model.\n",
    "    :return: A scikit-learn pipeline.\n",
    "    \"\"\"\n",
    "    assert model_type in model_dict, f\"Model {model_type} is not supported.\"\n",
    "\n",
    "    if model_params is None:\n",
    "        model_params = {}\n",
    "\n",
    "    model = model_dict[model_type](**model_params)\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('ohe', OneHotEncoder(), ['Pclass', 'Embarked']),\n",
    "        ('binarizer', OrdinalEncoder(), ['Sex']),\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "    return make_pipeline(preprocessor, model)\n",
    "\n",
    "\n",
    "def train_and_save_model(X: pd.DataFrame, y: pd.Series, model_type: str, model_params: Optional[Dict[str, Any]] = None):\n",
    "    \"\"\"\n",
    "    Trains a model, saves it to a file, and returns the trained model.\n",
    "\n",
    "    :param X: Feature data for training.\n",
    "    :param y: Target labels for training.\n",
    "    :param model_type: Type of model ('logistic_regression', 'svm', 'random_forest', etc.).\n",
    "    :param model_params: Hyperparameters for the model.\n",
    "    :return: The trained model.\n",
    "    \"\"\"\n",
    "    model = create_model(model_type, model_params)\n",
    "\n",
    "    model.fit(X, y)\n",
    "\n",
    "    model_file = os.path.join(MODEL_DIR, f\"{model_type}_model.pkl\")\n",
    "    with open(model_file, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    print(f\"Model trained and saved as {model_file}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_model(model_type: str):\n",
    "    \"\"\"\n",
    "    Loads a trained model from the saved file.\n",
    "\n",
    "    :param model_type: Type of model ('logistic_regression', 'svm', etc.).\n",
    "    :return: The trained model.\n",
    "    \"\"\"\n",
    "    model_file = os.path.join(MODEL_DIR, f\"{model_type}_model.pkl\")\n",
    "    if os.path.exists(model_file):\n",
    "        with open(model_file, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        print(f\"Model {model_type} loaded successfully.\")\n",
    "        return model\n",
    "    else:\n",
    "        print(f\"Model {model_type} does not exist.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f14d6d-0a9c-47b6-adc0-94f02f80e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    # Load your dataset\n",
    "    train_file = '../data/titanic/titanic_train.csv'\n",
    "    test_file =  '../data/titanic/titanic_test.csv'\n",
    "\n",
    "    X_train, y_train = validate_and_prepare_data(train_file, train=True)\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = split_train_test(X_train, y_train)\n",
    "    X_test = validate_and_prepare_data(test_file, train=False)\n",
    "\n",
    "    # Train and save Logistic Regression model\n",
    "    log_reg_model = train_and_save_model(X_train, y_train, 'logistic_regression', model_params={'C': 0.5, 'max_iter': 200})\n",
    "\n",
    "    # Train and save SVM model\n",
    "    svm_model = train_and_save_model(X_train, y_train, 'svm', model_params={'C': 1, 'kernel': 'linear'})\n",
    "\n",
    "    # Train and save Random Forest model\n",
    "    rf_model = train_and_save_model(X_train, y_train, 'random_forest', model_params={'n_estimators': 100, 'max_depth': 10})\n",
    "\n",
    "    # Train and save KNN model\n",
    "    knn_model = train_and_save_model(X_train, y_train, 'knn', model_params={'n_neighbors': 3, 'algorithm': 'auto'})\n",
    "\n",
    "    # Train and save Decision Tree model\n",
    "    dt_model = train_and_save_model(X_train, y_train, 'decision_tree', model_params={'max_depth': 5})\n",
    "\n",
    "    # Load the Logistic Regression model\n",
    "    log_reg_model_loaded = load_model('logistic_regression')\n",
    "\n",
    "    # Load the SVM model\n",
    "    svm_model_loaded = load_model('svm')\n",
    "\n",
    "    # Load the Random Forest model\n",
    "    rf_model_loaded = load_model('random_forest')\n",
    "\n",
    "    # Load the KNN model\n",
    "    knn_model_loaded = load_model('knn')\n",
    "\n",
    "    # Load the Decision Tree model\n",
    "    dt_model_loaded = load_model('decision_tree')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
